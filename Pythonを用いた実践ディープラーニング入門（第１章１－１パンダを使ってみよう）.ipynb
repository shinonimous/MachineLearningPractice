{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Pythonを用いた実践ディープラーニング入門（第１章１－１パンダを使ってみよう）.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinonimous/MachineLearningPractice/blob/main/Python%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E5%AE%9F%E8%B7%B5%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%85%A5%E9%96%80%EF%BC%88%E7%AC%AC%EF%BC%91%E7%AB%A0%EF%BC%91%EF%BC%8D%EF%BC%91%E3%83%91%E3%83%B3%E3%83%80%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%82%88%E3%81%86%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Q3LHAf-HW-"
      },
      "source": [
        "# Pythonを用いた実践ディープラーニング入門\n",
        "* 出典(Reference): [Jeff Heaton's Washington University Class Material](https://github.com/jeffheaton/t81_558_deep_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcQu5WDj-HW_"
      },
      "source": [
        "# 第１章　パンダを用いたデータの扱い方\n",
        "\n",
        "* **1.1: パンダを使ってみよう** [[Video]](https://currentlypreparingNo1) [[Notebook]](Pythonを用いた実践ディープラーニング入門（第１章１－１パンダを使ってみよう）.ipynb)\n",
        "* 1.2: パンダを用いたデータの分類 [[Video]](https://currentlypreparingNo2) [[Notebook]](CurrentlyPreparing.ipynb)\n",
        "* 1.3: パンダを用いたグループ化、ソート、シャッフル [[Video]](https://currentlypreparingNo3) [[Notebook]](CurrentlyPreparing.ipynb)\n",
        "* 1.4: パンダのApplyとMapを使ってみよう [[Video]](https://currentlypreparingNo4) [[Notebook]](CurrentlyPreparing.ipynb)\n",
        "* 1.5: パンダで特徴量エンジニアリング [[Video]](https://currentlypreparingNo5) [[Notebook]](CurrentlyPreparing.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2AZnaNV-HXA"
      },
      "source": [
        "# Google Colaboratory の設定確認\n",
        "\n",
        "以下のコードで、Google ColaboratoryのTensorflowのバージョンが正しいかどうかを確かめることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41gCh32G-HXA"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"正しいバージョンです\")\n",
        "except:\n",
        "    print(\"誤ったバージョンです\")\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVPiQXHm-HXC"
      },
      "source": [
        "# 1.1: パンダを使ってみよう\n",
        "\n",
        "パンダ([Pandas](http://pandas.pydata.org/))は、Pythonプログラミングにおいて利用できるオープンソースのライブラリで、非常に性能が良く、使い勝手の良いものです。そのコンセプトはR言語と通じるものがあります。通常、データ分析を目的として収集した生のデータは、そのままではニューラルネットワークで処理できる形式では無いため、パンダのようなライブラリを用いて、処理しやすいデータ形式に加工することになります。\n",
        "\n",
        "ここでは、まず分析のための[データセット](https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv)を手に入れ、Pandasのコンポーネントである[Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)にセットしてみましょう。\n",
        "\n",
        "ここで使うデータセットは、[Kaggle](https://www.kaggle.com/)で「Earthquake in Japan」と検索して見つけたものです。\n",
        "\n",
        "このデータセットは21列、14,093行から成ります。\n",
        "特に重要な列としては、latitude（緯度）、longitude（経度）、depth（震源の深さ）、mag（マグニチュード）があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ENhG0K-HXD"
      },
      "source": [
        "# Pandasを用いてデータセットを手に入れ、Dataframeにセットし、5行目迄を表示\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\")\n",
        "print(df[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m34rcHsk-HXE"
      },
      "source": [
        "上では**print**メソッドを利用してdataframe内のデータを表示しましたが、より美しく表示する方法があります。それが、**display**メソッドです。\n",
        "\n",
        "表示する際の列数、行数を指定したうえで表示できます。ここでは、11列、10行を表示してみましょう。\n",
        "\n",
        "※全行列を表示するには、0を指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdgp7yqt-HXE"
      },
      "source": [
        "pd.set_option('display.max_columns', 11)\n",
        "pd.set_option('display.max_rows', 10)\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8WnCzy-HXF"
      },
      "source": [
        "## 統計情報の取得と表示\n",
        "\n",
        "dataframeの統計情報を取得します。\n",
        "dataframe内の、int型、float型の列のみを対象として取り出し、その列の、列名、平均値、分散、標準偏差を一覧として表示してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-dXgt43-HXF"
      },
      "source": [
        "# int型、float型の値が入っている列のみを対象として取り出す\n",
        "df = df.select_dtypes(include=['int', 'float'])\n",
        "\n",
        "# 列名を一覧化してheadersに格納する\n",
        "headers = list(df.columns.values)\n",
        "\n",
        "# 一覧表示用のデータを格納する配列\n",
        "fields = []\n",
        "\n",
        "# 列名をひとつずつ取り出し、列名、平均値、分散、標準偏差を求めて一覧化する\n",
        "for header in headers:\n",
        "    fields.append({\n",
        "        'name' : header,\n",
        "        'mean': df[header].mean(),\n",
        "        'var': df[header].var(),\n",
        "        'sdev': df[header].std()\n",
        "    })\n",
        "\n",
        "for field in fields:\n",
        "    print(field)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4n1Op9c-HXG"
      },
      "source": [
        "上で作成した一覧は、JSON形式のデータのような見た目となっています。\n",
        "\n",
        "※正確なJSONではない（ひとつずつが配列要素なので）ので、ここでは疑似JSON形式と呼びます。本当のJSONファイルにするためには、上記の配列要素をひとつのリスト内にaddしてゆき、そのうえでPython JSON Libraryの**dumps**コマンドを実行しますが、ここでは割愛します。\n",
        "\n",
        "pd.read_csv(<ファイルパス>)によってdataframeを作成できることは実践しましたね。疑似JSON形式からdataframeを作成することもできます。ここではfieldsをdataframeに変換して、displayメソッドで表示してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w83505d8-HXG"
      },
      "source": [
        "pd.set_option('display.max_columns', 0)\n",
        "pd.set_option('display.max_rows', 0)\n",
        "df2 = pd.DataFrame(fields)\n",
        "display(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaJwHJBc-HXH"
      },
      "source": [
        "## 欠測値への対応\n",
        "\n",
        "理想的には、全ての列について、全ての行のデータが存在しているというのが望ましい状況ですが、現実的にはそうはいきません。\n",
        "\n",
        "欠測値への対応方法はいくつか考えられますが、簡単な方法として、中央値によって値を埋めるという方法が考えられます。\n",
        "\n",
        "まずは、欠測値が存在するのかどうかを調べてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QhIB-W2-HXH"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# デフォルトの欠測値にあわせて、NAや?も欠測値として定義する\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# Dataframeをそのままループを回すと列名が取得される\n",
        "for column_name in df:\n",
        "  print(f\"「{column_name}」は欠測値を持つか？: {pd.isnull(df[column_name]).values.any()}\")\n",
        "  if pd.isnull(df[column_name]).values.any():\n",
        "    print(f\"「{column_name}」の欠測値がある行は以下：\")\n",
        "    print(f\"{df[df[column_name].isnull()][column_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBXiHAa_uCLW"
      },
      "source": [
        "それでは、欠測値について該当列の中央値によって埋めてゆきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vu3ECiluMdY"
      },
      "source": [
        "for column_name in df:\n",
        "  print(f\"「{column_name}」は欠測値を持つか？: {pd.isnull(df[column_name]).values.any()}\")\n",
        "  if pd.isnull(df[column_name]).values.any():\n",
        "    print(\"　　欠測値を中央値によって埋めています…\")\n",
        "    med = df[column_name].median()\n",
        "    df[column_name] = df[column_name].fillna(med)\n",
        "    print(f\"　　「{column_name}」は欠測値を持つか？: {pd.isnull(df[column_name]).values.any()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-mGQdc3-HXH"
      },
      "source": [
        "## 外れ値への対応\n",
        "\n",
        "外れ値とは、大きすぎたり小さすぎたりする値のことで、データ観測上のエラーである可能性が考えられ、分析結果に大きなゆがみを生じさせてしまう可能性があるため、データセットから除外することを考えます。\n",
        "\n",
        "通常、平均値から標準偏差の何倍分か外れている、といった基準で外れ値を特定します。以下が外れ値を除外するコーディングの一例です。\n",
        "@引数\n",
        "- df    ：dataframe\n",
        "- name ：列名\n",
        "- sd    ：何倍分外れた時に除外するか、という倍数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-cNpKls-HXH"
      },
      "source": [
        "# 外れ値が存在する行について取り除くメソッド\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean())\n",
        "                          >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctJIBwy4-HXI"
      },
      "source": [
        "以下が、実際に外れ値が存在する行を取り除くコーディングの一例です。\n",
        "ここでは、平均値から標準偏差の2倍以上離れた場合に、外れ値とみなします。外れ値に関しては、magに関してのみチェックします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNPLdf5Q-HXI"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# 欠測値への対処（ここではdepthのみ対応）\n",
        "med = df['depth'].median()\n",
        "df['depth'] = df['depth'].fillna(med)\n",
        "\n",
        "# mag(マグニチュード)の外れ値をチェック\n",
        "print(\"外れ値を落とす前のDataframeの行数: {}\".format(len(df)))\n",
        "remove_outliers(df,'mag',2)\n",
        "print(\"外れ値を落とした後のDataframeの行数: {}\".format(len(df)))\n",
        "\n",
        "pd.set_option('display.max_columns', 0)\n",
        "pd.set_option('display.max_rows', 5)\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IroB_7JD-HXI"
      },
      "source": [
        "## 利用しない列への対応\n",
        "\n",
        "ニューラルネットワークにおいて利用しない列は削除します。今回、latitude（緯度）、longitude（経度）、depth（震源の深さ）、mag（マグニチュード）以外は利用しないものとし、dropしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgYVzQJy-HXI"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "print(f\"列を落とす前: {list(df.columns)}\")\n",
        "drop_col = ['time','magType','gap','dmin','rms','net','id','updated','place','type','horizontalError','depthError','magError','magNst','status','locationSource','magSource']\n",
        "df.drop(drop_col, axis=1, inplace=True)\n",
        "print(f\"列を落とした後: {list(df.columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MTL-qsg-HXJ"
      },
      "source": [
        "## 行や列の結合\n",
        "上記ではdropすることで利用する列のみを得ましたが、逆に利用する列のみを選んで、**concat**メソッドによって結合することで、利用する列を作成できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VUoF6us-HXJ"
      },
      "source": [
        "# latitude（緯度）、longitude（経度）、depth（震源の深さ）、mag（マグニチュード）を選んで結合\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "col_latitude = df['latitude']\n",
        "col_longitude = df['longitude']\n",
        "col_depth = df['depth']\n",
        "col_mag = df['mag']\n",
        "result = pd.concat([col_latitude, col_longitude, col_depth, col_mag], axis=1)\n",
        "\n",
        "pd.set_option('display.max_columns', 0)\n",
        "pd.set_option('display.max_rows', 5)\n",
        "display(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgU3JBU5-HXK"
      },
      "source": [
        "**concat**メソッドは、axisに0を指定することで行を結合することもできます。以下では、dataframeの上から2行、下から2行のみを取得して表示してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipp0ektF-HXK"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "result = pd.concat([df[0:2],df[-2:]], axis=0)\n",
        "\n",
        "pd.set_option('display.max_columns', 7)\n",
        "pd.set_option('display.max_rows', 0)\n",
        "display(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIFBk5_q-HXK"
      },
      "source": [
        "## 訓練データとテストデータの分割\n",
        "\n",
        "機械学習のモデルは初見のデータからどれだけ正確に予測ができるか、によって評価することができます。そのため、手に入っているデータの全てをモデルの構築のための訓練データとして利用するのではなく、一部を訓練データ、一部をテストデータ(初見のデータ)\n",
        "として分割することが良くあります。\n",
        "* **訓練データ** - 機械学習のモデルを構築するために用いる。\n",
        "* **テストデータ** - 構築したモデルを評価するために用いる。\n",
        "\n",
        "訓練データとテストデータの分割方法として、有名な二つの方法があります。\n",
        "* **データ分割** - 定められた比率でデータを分割します。一般的な比率としては、80%を訓練データ、20%をテストデータとします。\n",
        "* **K分割交差テスト** - データ分割の応用のような形式です。データをK分割します。たとえば、データを5分割したとします。そのうち、4つを訓練データ、1つをテストデータとして用います。これだけだと、80%を訓練データ、20%をテストデータとして扱う「データ分割」と同じなのですが、単なる「データ分割」との違いは、テストデータとして利用する1分割分について、全てのパターンを実施するということです。5分割する例だと、どれをテストデータとして利用するかによって、5パターン分のモデルを構築することができます。最終的に5つのモデルの精度を評価し、平均化して一つのモデルに融合します。\n",
        "\n",
        "以下に紹介するコーディングは、80%を訓練データ、20%をテストデータとするデータ分割を実施するものです。\n",
        "\n",
        "**Figure 2.TRN-VAL: Training and Validation**\n",
        "![Training and Validation](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_1_train_val.png \"Training and Validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Gto6BQ-HXK"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Dataframeをシャッフル\n",
        "df = df.reindex(np.random.permutation(df.index)) \n",
        "\n",
        "# np.random.rand(len(df))は0～1の範囲で、len(df)個の数値をランダムに生成する。\n",
        "# 生成値と0.8を比較することで、生成された値が0.8より小さい場合にはmaskはtrueとなり、大きい場合はfalseとなる。\n",
        "# mask[0] = true, mask[1] = false, … , mask[len(df)] = false、といった配列が生成されているということ。\n",
        "mask = np.random.rand(len(df)) < 0.8\n",
        "\n",
        "# maskに基づいてデータを分割する。maskがtrueとなるdfの行はtrainDFに含まれる。\n",
        "trainDF = pd.DataFrame(df[mask])\n",
        "\n",
        "# ~maskはmaskの値を反転（tureをfalseに、falseをtrueに）する。\n",
        "validationDF = pd.DataFrame(df[~mask])\n",
        "\n",
        "print(f\"訓練データフレームの行数: {len(trainDF)}\")\n",
        "print(f\"テストデータフレームの行数: {len(validationDF)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKITfXNY-HXL"
      },
      "source": [
        "## データフレームのマトリックス（行列）への変換\n",
        "\n",
        "ニューラルネットワークは、PythonのDataframeを扱うことができないので、行列形式に変換してあげる必要があります。ここでは、Dataframeの**values**プロパティを用いることで、Dataframeをマトリックスに変換しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69kdh8Je-HXL"
      },
      "source": [
        "print(df)\n",
        "print(df.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8lSS6RI-HXL"
      },
      "source": [
        "列名を指定することで、特定の列のみのマトリックスを作成することもできます。以下のように指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9bOCHnR-HXL"
      },
      "source": [
        "df[['latitude', 'longitude', 'depth', 'mag']].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdVUDdTg-HXL"
      },
      "source": [
        "## DataframeをCSV形式で保存する\n",
        "\n",
        "PandasのDataframeはCSV形式で保存することができます。行をシャッフルして、CSVファイルとして保存してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3QnAzs-HXM"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = \".\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "filename_write = os.path.join(path, \"JapanEarthquake_Shuffle.csv\")\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "# index = falseの指定は、行名をファイルに含めないということを意味する。\n",
        "df.to_csv(filename_write, index=False) \n",
        "print(\"保存しました。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnWtBqcU-HXM"
      },
      "source": [
        "## DataframeをPickle形式で保存する\n",
        "\n",
        "[Pickle](https://docs.python.org/3/library/pickle.html)形式はPythonのデータ保存形式です。Dataframeを保存する際には、Python以外のプログラムでも利用できるCSV形式で保存する場合が多いですが、CSV形式で保存すると、Pickleより処理速度が遅いのと、形式変換の際にわずかながら情報が落ちるため、Pythonでしか利用しないのであれば、Pickle形式に保存すると良いでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdLuPva8-HXM"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "path = \".\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://shinonimous.sakura.ne.jp/csv/Japan%20earthquakes%202001%20-%202018.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "filename_write = os.path.join(path, \"JapanEarthquake_Shuffle.pkl\")\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "with open(filename_write,\"wb\") as fp:\n",
        "    pickle.dump(df, fp)\n",
        "\n",
        "print(\"保存しました。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y3bu0Ja-HXM"
      },
      "source": [
        "Pickleを再度メモリにロードする方法を説明します。Pickleからデータをロードした場合、index番号（行番号）が、前回Shuffleした状態からそのままでロードされます。CSV形式からロードする場合には、このindex番号は残っていません（新たに採番されます）。CSV形式にするとわずかに落ちる情報とは、こうした情報です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTO8zQ7X-HXM"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "path = \".\"\n",
        "\n",
        "filename_read = os.path.join(path, \"JapanEarthquake_Shuffle.pkl\")\n",
        "\n",
        "with open(filename_read,\"rb\") as fp:\n",
        "    df = pickle.load(fp)\n",
        "\n",
        "pd.set_option('display.max_columns', 7)\n",
        "pd.set_option('display.max_rows', 5)\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}